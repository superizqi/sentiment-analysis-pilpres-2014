# -*- coding: utf-8 -*-
"""DSI Melek Sentiment Post Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X-sxTagYStJ9lDPJ5aVyuFbsz-3gcnAH

#**Twitter Sentiment Analysis During Indonesia Presidential Election 2014**
Post-Test for DSI Melek Sentiment Online Workshop
"""

from google.colab import drive
drive.mount('/content/drive')

HOME_DIR = '/content/drive/My Drive/dataset/dsi_sentimen/'

import pandas as pd

"""**Load Data Tweet**"""

df = pd.read_csv(HOME_DIR+'data/Capres2014-2.0.csv',encoding='latin1')
df_ori = df.copy()
df = df[['Isi_Tweet','Sentimen']]

df['Isi_Tweet'].head(10).values

"""**Load Data Stopwords**"""

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
sw = stopwords.words('indonesian')
sw.extend(stopwords.words('english'))

# load stopwords indonesia
stp = open(HOME_DIR+'data/twitter_stp.dic')
stp_words = []
item = stp.readline()
while item != '':
  stp_words.append(item[:-1])
  item = stp.readline()

stp_words[0:10]

len(stp_words)

sw += stp_words
len(sw)

sw = set(sw)

len(sw)

"""**Load Data Kata Singkatan**"""

# load data kata singkatan
stp = open(HOME_DIR+'data/singkatankata.dic')
kata_asli = []
singkatan = []
dict_singkatan = {}
item = stp.readline()
while item != '':
  singkatan.append(item[:-1].split('\t')[0])
  kata_asli.append(item[:-1].split('\t')[-1])
  dict_singkatan[item[:-1].split('\t')[0].lower()] = item[:-1].split('\t')[-1].lower()
  item = stp.readline()

# dict_singkatan

# kata_asli[0:5]

# singkatan[0:5]

"""**Load Data Kata Dasar dan Sentimen**"""

df_seed = pd.read_csv(HOME_DIR+'data/seed_all.csv')
df_seed.columns = df.columns

df_seed.head()

df_seed['Sentimen'].value_counts()

dict_label = {'Sentimen':{'positif': 1,
                            'negatif': -1}}

df_seed.replace(dict_label, inplace=True)
df_seed['Sentimen'].value_counts()

df_seed.head()

len(df_seed)

len(df)

df_process = df.append(df_seed, ignore_index = True)

len(df_process)

df_process['Sentimen'].value_counts()

# mengubah semua value ke lowercase
df_process['Isi_Tweet'] = df_process['Isi_Tweet'].str.lower()

df_process.head()

# mengubah kata singkatan menjadi kata asli
def expand_dict(x, dct):
  tw = x.split()
  return ' '.join([dct.get(item, item) for item in tw])

df_process['Isi_Tweet'] = df_process['Isi_Tweet'].apply(lambda x: expand_dict(x,dict_singkatan))
df_process_original = df_process.copy()

"""**Inisiasi Vectorizer**"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
# vect = TfidfVectorizer(stop_words=sw)
# vect = CountVectorizer(stop_words=sw)
vect = TfidfVectorizer(min_df=0.005, max_df=0.99, stop_words=sw, token_pattern='\\b[a-zA-Z][a-zA-Z][a-zA-Z]+ \\b')


x = df_process['Isi_Tweet']
y = df_process['Sentimen']

# vectorizer
x = vect.fit_transform(x)

# secara default 75 25
x_train, x_test, y_train, y_test = train_test_split(x,y,stratify=y)

"""**Inisiasi Model**"""

# machine learning model
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(x_train,y_train)

"""**Evaluasi Model**"""

from sklearn.metrics import classification_report
y_pred = model.predict(x_test)

print(classification_report(y_pred,y_test))

from sklearn.model_selection import cross_val_score
# x = pd.DataFrame(x.todense(), columns=vect.get_feature_names())
cross_val_score(model,x,y,cv=5).mean()

"""**Hyperparameter Tuning**"""

# from sklearn.model_selection import GridSearchCV
# param_grid = {'criterion' : ['gini','entropy'],
#               'max_features' : ['auto','sqrt','log2'],
#               'n_estimators' : [100,200,300],
#               'class_weight' : ['balanced','balanced_subsample']}x  
# gsv = GridSearchCV(RandomForestClassifier(),param_grid=param_grid,n_jobs=4,cv=5)
# gsv.fit(x,y)
# print(gsv.best_params_)
# print(gsv.best_score_)

"""**Inisiasi Model dengan hasil dari hyperparameter tuning**"""

model = RandomForestClassifier(class_weight='balanced',
                               criterion = 'entropy',
                               max_features = 'sqrt',
                               n_estimators = 300)
# model = RandomForestClassifier(**gsv.best_params_)
model.fit(x_train,y_train)

"""**Evaluasi Model**"""

y_pred = model.predict(x_test)
print(classification_report(y_pred,y_test))

cross_val_score(model,x,y,cv=5).mean()

"""Hasil akhir yang didapatkan akurasinya cukup rendah kedepannya mungkin bisa dikembangkan dengan **menggunakan lebih banyak data** lagi dan juga menggunakan **algoritma lain** dengan tujuan untuk mengetahui mana hasil yang terbaik.

**Pipeline**
"""

from sklearn.pipeline import Pipeline
# bisa otomatis fit
pipeline = Pipeline([
                     ('vect',vect),
                     ('clf',model)
])

x = df_process['Isi_Tweet']
y = df_process['Sentimen']

# secara default 75 25
x_train, x_test, y_train, y_test = train_test_split(x,y,stratify=y)

pipeline.fit(x_train,y_train)

y_pred = pipeline.predict(x_test)
print(classification_report(y_pred,y_test))

"""**Export pickle**"""

import pickle

# dump pipeline to pickle
pickle.dump(pipeline,open('sentiment_model.pkl', 'wb'))

# load the model from disk
loaded_model = pickle.load(open('sentiment_model.pkl', 'rb'))
loaded_model.fit(x_train,y_train)

y_pred = pipeline.predict(x_test)
print(classification_report(y_pred,y_test))

result = loaded_model.score(x_test,y_test)
print(result)

result = loaded_model.score(x_train,y_train)
print(result)

# End

"""#**Visualisasi**"""

# df_process_original.copy().head()
dfp = df_process_original.copy()

list_user = []
list_word = []
import re
for i in dfp['Isi_Tweet']:
  for j in i.split():
    if '@' in j:
      list_user.append(j.replace(':',''))
    else:
      if j != '':
        list_word.append(re.sub('[^A-Za-z0-9]+', '', j))
len(list_user)

import matplotlib.pyplot as plt
import pandas as pd
srs = pd.DataFrame({'username' : list_user})['username'].value_counts()
srs.head(5).sort_values(ascending=True).plot(kind='barh',figsize=(15,5),title='Akun Twitter Yang Paling Sering Disebut')

srs = df['Sentimen'].value_counts().rename({0: 'Neutral', 1: 'Positive',-1:'Negative'})
srs.plot(kind='bar',title='Sentimen',rot=0)

lw = []
for i in list_word:
  if i not in sw:
    lw.append(i)
len(lw)
srs = pd.DataFrame({'word' : lw})['word'].value_counts().drop(labels=[''])
srs.head(10).sort_values(ascending=True).plot(kind='barh',figsize=(15,5),title='Kata yang paling sering muncul')

from wordcloud import WordCloud

dff = pd.DataFrame({'word' : lw})

wordcloud2 = WordCloud(width=800, height=400).generate(' '.join(dff['word']))
plt.imshow(wordcloud2)
plt.axis("off")
plt.show()

srs.index

df.Sentimen.values

import numpy as np
np.array(lw)

len(list_word)

"""**Experiment**"""

aa = ' '.join(df['Isi_Tweet'].loc[df['Sentimen'] == -1].values.tolist())
bb = ' '.join(df['Isi_Tweet'].loc[df['Sentimen'] == 1].values.tolist())
cc = ' '.join(df['Isi_Tweet'].loc[df['Sentimen'] == 0].values.tolist())
dd = ' '.join(df_seed['Isi_Tweet'].loc[df_seed['Sentimen'] == -1][0:500].values.tolist())

dd

# inverence
new_tweet = [aa,
             bb,
             cc,
             dd,
             'pedih masam memalukan membenci kebencian mengerikan kasar pelupa penyalahgunaan']
pipeline.predict(new_tweet)

loaded_model.predict(new_tweet)

df_seed['Isi_Tweet'].loc[df['Sentimen'] == -1][0:10]

"""**Trash**"""

# vect.get_feature_names()
# srs.head()
# srs = pd.DataFrame(x.todense(), columns=vect.get_feature_names())
# # # load the model from disk
# # loaded_model = pickle.load(open(HOME_DIR+'sentiment_model.pkl', 'rb'))
# # result = loaded_model.score(df['Isi_Tweet'][0:200],df['Sentimen'][0:200])
# # print(result)
# # import pickle
# # # dump pipeline to pickle
# # pickle.dump(pipeline,open(HOME_DIR+'sentiment_model.pkl', 'wb'))
# # # pickle.dump(pipeline,open('sentiment_model.pkl', 'wb'))
# from sklearn.metrics import classification_report
# print(classification_report(y_pred,y_test))
# y_pred = pipeline.predict(x_test)
# pipeline.fit(df['Isi_Tweet'],df['Sentimen'])
# # pipeline.fit(x_train,y_train)
# param_grid = {'n_neighbors': np.arange(3,100)}
# gsv = GridSearchCV(KNeighborsClassifier(),param_grid=param_grid,n_jobs=4,cv=5)
# gsv.fit(x,y)
# print(gsv.best_params_)
# print(gsv.best_score_)
# model = RandomForestClassifier(criterion='entropy',max_features='auto')
# model.fit(x,y)
# from sklearn.metrics import classification_report
# pipeline.fit(x_train,y_train)

# y_pred = pipeline.predict(x_test)
# from sklearn.metrics import classification_report
# print(classification_report(y_pred,y_test))